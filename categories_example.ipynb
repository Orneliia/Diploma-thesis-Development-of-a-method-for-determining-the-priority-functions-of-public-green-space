{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import re\n",
    "from razdel import tokenize\n",
    "import pymorphy3\n",
    "from nltk.tokenize import word_tokenize\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON files\n",
    "final_gdf = gpd.read_file(\"final.geojson\")\n",
    "final_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация морфологического анализатора\n",
    "morph = pymorphy3.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список русских стоп-слов\n",
    "russian_stopwords = [\n",
    "    \"и\", \"в\", \"во\", \"не\", \"что\", \"он\", \"на\", \"я\", \"с\", \"со\", \"как\", \"а\", \"то\", \"все\", \n",
    "    \"она\", \"так\", \"его\", \"но\", \"да\", \"ты\", \"к\", \"у\", \"же\", \"вы\", \"за\", \"бы\", \"по\", \n",
    "    \"только\", \"ее\", \"мне\", \"было\", \"вот\", \"от\", \"меня\", \"еще\", \"нет\", \"о\", \"из\", \n",
    "    \"ему\", \"теперь\", \"когда\", \"даже\", \"ну\", \"вдруг\", \"ли\", \"если\", \"уже\", \"или\", \n",
    "    \"ни\", \"быть\", \"был\", \"него\", \"до\", \"вас\", \"нибудь\", \"опять\", \"уж\", \"вам\", \n",
    "    \"ведь\", \"там\", \"потом\", \"себя\", \"ничего\", \"ей\", \"может\", \"они\", \"тут\", \"где\", \n",
    "    \"есть\", \"надо\", \"ней\", \"для\", \"мы\", \"тебя\", \"их\", \"чем\", \"была\", \"сам\", \n",
    "    \"чтоб\", \"без\", \"будто\", \"чего\", \"раз\", \"тоже\", \"себе\", \"под\", \"будет\", \"ж\", \n",
    "    \"тогда\", \"кто\", \"этот\", \"того\", \"потому\", \"этого\", \"какой\", \"совсем\", \"ним\", \n",
    "    \"здесь\", \"этом\", \"один\", \"почти\", \"мой\", \"тем\", \"чтобы\", \"нее\", \"сейчас\", \n",
    "    \"были\", \"куда\", \"зачем\", \"всех\", \"никогда\", \"можно\", \"при\", \"наконец\", \n",
    "    \"два\", \"об\", \"другой\", \"хоть\", \"после\", \"над\", \"больше\", \"тот\", \"через\", \n",
    "    \"эти\", \"нас\", \"про\", \"всего\", \"них\", \"какая\", \"много\", \"разве\", \"три\", \n",
    "    \"эту\", \"моя\", \"впрочем\", \"хорошо\", \"свою\", \"этой\", \"перед\", \"иногда\", \n",
    "    \"лучше\", \"чуть\", \"том\", \"нельзя\", \"такой\", \"им\", \"более\", \"всегда\", \n",
    "    \"конечно\", \"всю\", \"между\", \"это\", \"ломоносов\", \"vk\", \"метро\", \"европа\",\n",
    "    \"парк\", \"санкт-петербург\", \"санкт\", \"петербург\", \"очень\", \"здравствуйте\", \n",
    "    \"время\", \"год\", \"город\", \"https\", \"также\", \"id\", \"который\", \"всё\", \"весь\", \"kgainfo\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, stop_words=None):\n",
    "    \"\"\"\n",
    "    Препроцессинг текста: токенизация, приведение к нижнему регистру, лемматизация, удаление стоп-слов и пунктуации.\n",
    "    :param text: Исходный текст.\n",
    "    :param stop_words: Список стоп-слов для удаления (по умолчанию None).\n",
    "    :return: Лемматизированный текст.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    # Токенизация с помощью razdel\n",
    "    tokens = [token.text.lower() for token in tokenize(text)]\n",
    "    # Лемматизация\n",
    "    lemms = [morph.parse(token)[0].normal_form for token in tokens if token.isalpha()]\n",
    "    # Удаление стоп-слов\n",
    "    if stop_words:\n",
    "        lemms = [word for word in lemms if word not in stop_words]\n",
    "    return \" \".join(lemms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция очистки: оставляем только русские символы и удаляем ссылки и то, что не смогли удалить другии путями\n",
    "def clean(text):\n",
    "    emoji_pattern = re.compile(pattern = \"[\"\n",
    "                               u\"\\U00000000-\\U00000009\" #управляющие символы (control characters)\n",
    "                                u\"\\U0000000B-\\U0000001F\" #управляющие символы (control characters)\n",
    "                                u\"\\U00000041-\\U0000007A\" #буквы английского алфавита\n",
    "                                u\"\\U00000080-\\U00000400\" #специальные символы и цифры\n",
    "                                u\"\\U00000402-\\U0000040F\" #нерусские символы кириллицы\n",
    "                                u\"\\U00000452-\\U0010FFFF\" #нерусские символы\n",
    "                                u\"/\"                     #косая черта вправо\n",
    "                                u\"|\"                     #вертикальная черта\n",
    "                                u\":\"                     #двоеточие\n",
    "                                \"]+\", flags = re.UNICODE)\n",
    "\n",
    "    url_pattern = re.compile(r'http\\S+')\n",
    "\n",
    "    #удаляем ссылки (URL) из данных\n",
    "    text_without_urls = url_pattern.sub(r'', text)\n",
    "    cleaned_text = emoji_pattern.sub(r'', text_without_urls)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gdf[\"processed_text\"] = final_gdf[\"text\"].apply(lambda x: preprocessing(x, stop_words=russian_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gdf[\"processed_text\"] = final_gdf[\"processed_text\"].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"Функционально-структурные показатели\": [\n",
    "        \"удобство\", \"лавочки\", \"пруд\",\n",
    "        \"детская\", \"отдых\", \"дорожки\", \"фонтан\",\n",
    "        \"спорт\", \"прогулки\", \"тихо\"\n",
    "    ],\n",
    "    \"Урботехнические показатели\": [\n",
    "        \"доступ\", \"рядом\", \"транспорт\", \"парковка\",\n",
    "        \"шум\", \"комфорт\", \"далеко\", \"недоступно\"\n",
    "    ],\n",
    "    \"Экологические показатели\": [\n",
    "        \"воздух\", \"дышать\", \"свежо\", \"растения\",\n",
    "        \"деревья\", \"кусты\", \"трава\", \"зелень\",\n",
    "        \"чистый\", \"грязный\", \"ухоженный\", \"гнездо\",\n",
    "        \"эстетика\", \"цветы\", \"клумбы\", \"птицы\"\n",
    "    ],\n",
    "    \"Эксплуатационные показатели\": [\n",
    "        \"безопасность\", \"опасно\", \"свет\", \"камеры\",\n",
    "        \"поломка\", \"уход\", \"ремонт\", \"сломано\",\n",
    "        \"чистить\", \"убирают\", \"комфорт\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для проверки схожести слов\n",
    "def similar(a, b, threshold=0.9):\n",
    "    return SequenceMatcher(None, a, b).ratio() >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_keywords_unique_normalized_fixed(text_words, categories, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Функция подсчёта совпадений с учётом только уникальных совпадений и нормализацией.\n",
    "    \"\"\"\n",
    "    if not text_words:  # Если текст пустой\n",
    "        return {key: 0 for key in categories.keys()}\n",
    "\n",
    "    # Словарь для хранения уникальных совпадений\n",
    "    match_counts = {key: set() for key in categories.keys()}\n",
    "\n",
    "    for word in text_words:\n",
    "        for category, keywords in categories.items():\n",
    "            if any(similar(word, keyword.lower(), threshold) for keyword in keywords):\n",
    "                match_counts[category].add(word)  # Добавляем слово как уникальное совпадение\n",
    "\n",
    "    # Нормализация по количеству ключевых слов\n",
    "    normalized_counts = {\n",
    "        category: len(matches) / len(categories[category]) if len(categories[category]) > 0 else 0\n",
    "        for category, matches in match_counts.items()\n",
    "    }\n",
    "\n",
    "    print(\"Уникальные совпадения по категориям:\", {k: list(v) for k, v in match_counts.items()})\n",
    "    print(\"Нормализованные значения по категориям:\", normalized_counts)\n",
    "    return normalized_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение функции к каждому тексту в столбце processed_text\n",
    "final_gdf[\"normalized_counts\"] = final_gdf[\"text\"].apply(\n",
    "    lambda text: match_keywords_unique_normalized_fixed(text.split(), categories, threshold=0.9)\n",
    ")\n",
    "\n",
    "# Разделяем словарь на отдельные колонки\n",
    "normalized_counts_df = pd.DataFrame(final_gdf[\"normalized_counts\"].tolist(), index=final_gdf.index)\n",
    "\n",
    "# Добавляем новые колонки в DataFrame\n",
    "final_gdf = pd.concat([final_gdf, normalized_counts_df], axis=1)\n",
    "\n",
    "# Вывод первых строк для проверки\n",
    "print(final_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Колонки с нормализованными значениями категорий\n",
    "normalized_columns = [\n",
    "    \"Функционально-структурные показатели\",\n",
    "    \"Урботехнические показатели\",\n",
    "    \"Экологические показатели\",\n",
    "    \"Эксплуатационные показатели\"\n",
    "]\n",
    "\n",
    "# Группируем данные по `label` и рассчитываем средние значения\n",
    "grouped_data = final_gdf.groupby(\"label\")[normalized_columns].mean()\n",
    "\n",
    "# Построение графика\n",
    "ax = grouped_data.plot(kind=\"bar\", figsize=(10, 6), alpha=0.8, cmap=\"Set2\")\n",
    "ax.set_title(\"Распределение нормализованных значений по эмоциям\", fontsize=16)\n",
    "ax.set_xlabel(\"Эмоции\", fontsize=12)\n",
    "ax.set_ylabel(\"Среднее нормализованное значение\", fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Категории показателей\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "X = final_gdf[[\"Функционально-структурные показатели\", \"Урботехнические показатели\", \"Экологические показатели\", \"Эксплуатационные показатели\"]]\n",
    "y = final_gdf[\"emotion_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет корреляции и p-value для каждого признака\n",
    "correlations = []\n",
    "\n",
    "for column in X.columns:\n",
    "    corr, p_value = pearsonr(X[column], y)  # Вычисляем корреляцию и p-value\n",
    "    correlations.append({\"Feature\": column, \"Correlation\": corr, \"P-value\": p_value})\n",
    "\n",
    "# Преобразуем результаты в DataFrame\n",
    "correlation_df = pd.DataFrame(correlations).sort_values(by=\"Correlation\", ascending=False)\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Корреляции и p-value каждого признака с целевой переменной:\")\n",
    "correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение модели Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Важность признаков\n",
    "feature_importances = rf_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\": feature_importances\n",
    "}).sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация важности признаков\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df[\"feature\"], importance_df[\"importance\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Важность\", fontsize=12)\n",
    "plt.ylabel(\"Признаки\", fontsize=12)\n",
    "plt.title(\"Важность признаков по Random Forest\", fontsize=16)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
